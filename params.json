{"name":"Link Prediction (Supervised Random Walks)","tagline":"Predicting and recommending links in a Social Network.","body":"# Link Prediction:\r\nWe are given snapshot of a network and would like to infer which which interactions among existing\r\nmembers are likely to occur in the near future or which existing interactions are we missing. The challenge is to effectively combine the information from the network structure with rich node and edge\r\nattribute data.\r\n\r\n# Supervised Random Walks:\r\nThis repository is the implementation of Link prediction based on the paper Supervised Random Walks by  Lars Backstrom et al. The essence of which is that we combine the information from the network structure with the node and edge level attributes using Supervised Random Walks. We achieve this by using these attributes to guide a random walk on the graph. We formulate a supervised learning task where the goal is to learn a function that assigns strengths to edges in the network such that a random walker is more likely to visit the nodes to which new links will be created in the future. We develop an efficient training algorithm to directly learn the edge strength estimation function.\r\n\r\n# Problem Description:\r\nWe are given a directed graph _G(V,E)_, a node _s_ and a set of candidates to which _s_ could create an edge. We label nodes to which _s_ creates edges in the future as _destination nodes D = {d<sub>1</sub>,..,d<sub>k</sub>}_, while we call the other nodes to which s does not create edges no-link nodes _L = {l<sub>1</sub>,..,l<sub>n</sub>}_. We label candidate nodes with a set _C = D union L_. _D_ are positive training examples and _L_ are negative training examples. We can generalize this to multiple instances of _s, D, L_. Each node and each edge in G is further described with a set of features. We assume that each edge _(u,v)_ has a corresponding feature vector psi<sub>uv</sub> that describes u and v and the interaction attributes.\r\n\r\nFor each edge (u,v) in G we compute the strength _a<sub>uv</sub> = f<sub>w</sub>(psi<sub>uv</sub>)_. Function _f<sub>w</sub>_ parameterized by _w_ takes the edge feature vector _psi<sub>uv</sub>_ as input and computes the corresponding edge strength _a<sub>uv</sub>_ that models the random walk transition probability. It is exactly the function _f<sub>w</sub>(psi)_ we learn in the training phase of the algorithm.\r\n\r\nTo predict new edges to _s_, first edge strengths of all edges are calculated using _f<sub>w</sub>_. Then random walk with restarts is run from _s_. The stationary distribution _p_ of random walk assigns each node _u_ a probability _p<sub>u</sub>_. Nodes are ordered by _p<sub>u</sub>_ and top ranked nodes are predicted as future destination nodes to _s_. The task is to learn the parameters _w_ of function _f<sub>w</sub>(psi<sub>uv</sub>)_ that assigns each edge a transition probability. One can think of the weights _a<sub>uv</sub>_ as edge strengths and the random walk is more likely to traverse edges of high strength and thus nodes connected to node _s_ via paths of strong edges will likely to be visited by the random walk and will thus rank higher.\r\n\r\n## The optimization problem:\r\nThe training data contains information that source node _s_ will create edges to node _d subset D_ and not _l subset L_. So we set parameters _w_ of the function _f<sub>w</sub>(psi<sub>uv</sub>)_ so that it will assign edge weights _a<sub>uv</sub>_ in such a way that the random walk will be more likely to visit nodes in _D_ than _L_, _i.e.,_ _p<sub>l</sub> < p<sub>d</sub>_ for each _d subset D_ and _l subset L_. And thus we define the optimization problem as follows.\r\n![optimization problem hard version](http://i.imgur.com/zMjJ1Nb.png)\r\nwhere _p_ is the vector of pagerank scores. Pagerank scores _p<sub>i</sub>_ depend on edge strength on _a<sub>uv</sub>_ and thus actually depend on _f<sub>w</sub>(psi<sub>uv</sub>)_ which is parameterized by _w_. The above equation (1) simply states that we want to find _w_ such that the pagerank score of nodes in _D_ will be greater than the scores of nodes in _L_. We prefer the shortest _w_ parameters simply for the sake of regularization. But the above equation is the \"hard\" version of the optimization problem. However it is unlikely that a solution satisfying all the constraints exist. We make the optimization problem \"soft\" by introducing a loss function that penalizes the violated constraints. Now the optimization problem becomes,\r\n![optimization problem soft version.](http://i.imgur.com/oZ2pYN1.png)\r\nwhere lambda is the regularization parameter that trades off between the complexity(norm of _w_) for the fit of the model(how much the constraints can be violated). And _h(.)_ is a loss function that assigns a non-negative penalty according to the difference of the scores _p<sub>l</sub>-p<sub>d</sub>_. _h(.) = 0_ if _p<sub>l</sub> < p<sub>d</sub>_ as the constraint is not violated and _h(.) > 0_ if _p<sub>l</sub> > p<sub>d</sub>_\r\n\r\n### Solving the optimization problem:\r\nFirst we need to establish connection between the parameters _w_ and the random walk scores _p_. Then we show how to obtain partial derivatives of the loss function and _p_ with respect to _w_ and then perform gradient descent to obtain optimal values of _w_ and minimize loss.\r\nWe build a random walk stochastic transition matrix _Q<sup>'</sup>_ from the edge strengths _a<sub>uv</sub>_ calculated from _f<sub>w</sub>(psi<sub>uv</sub>)_.\r\n![Q dash](http://i.imgur.com/JiSHf7t.png)\r\nTo obtain the final random walk transition probability matrix _Q_, we also incorporate the restart probability _alpha_, _i.e.,_ the probability with which the random walk jumps back to seed node _s_, and thus \"restarts\".\r\n![Q](http://i.imgur.com/vE2P7LJ.png)\r\neach entry _Q<sub>uv</sub>_ deÔ¨Ånes the conditional probability that a walk will traverse edge (u, v) given that it is currently at node u.\r\nThe vector _p_ is the stationary distribution of the Random Walk with restarts(also known as Personalized Page Rank), and is the solution to the following eigen vector equation.\r\n![eigen vector equation](http://i.imgur.com/UFwnobA.png)\r\nThe above equation establishes the connection between page rank scores _p_ and the parameters _w_ via the random walk transition probability matrix _Q_. Our goal now is to minimize the soft version of the loss function(eq. 2) with respect to parameter vector _w_. We do this by obtaining the gradient of _F(w)_ with respect to _w_, and then performing gradient based optimization method to find _w_ that minimize _F(w)_. This gets complicated due to the fact that equation 4 is recursive. For this we introduce _delta<sub>ld</sub> = p<sub>l</sub>-p<sub>d</sub>_ and then we can write the derivative\r\n![delta ld](http://i.imgur.com/FhZVZEB.png)\r\nand then we can write the derivative of _F(w)_ as follows\r\n![lossfunction gradient with delta](http://i.imgur.com/oisE40X.png)\r\nFor commonly used loss functions _h(.)_ it is easy to calculate derivative, but it is not clear how to obtain partial derivatives of _p_ wrt _w_. _p_ is the principle eigen vector of matrix _Q_. The above eigen vector equation can also be written as follows.\r\n![eigen vector reduced form.](http://i.imgur.com/z00CXm4.png)\r\nand taking the derivatives now gives\r\n![derivative of p recursive form](http://i.imgur.com/FhZVZEB.png)\r\nabove _p<sub>u</sub>_ and its partial derivative are entangled in the equation, however we compute the above values iteratively as follows\r\n![power method to compute p and its partial derivative iteratively.](http://i.imgur.com/YwrBiIq.png)\r\nwe initialize the vector _p_ as _1/|V|_ and all its derivatives as zeroes before the iteration starts and terminates the recursion till the _p_ and its derivatives converge for an epsilon say _10e-12_.\r\nTo solve equation 4, we need partial derivative of _Q<sub>ju</sub>, this calculation is straight forward. When _(j,u) subset E_\r\n![partial derivative of Qju](http://i.imgur.com/aLDlWP4.png)\r\n\r\n# My Implementation:\r\nhttp://imgur.com/FhZVZEB,oisE40X,aLDlWP4,z00CXm4,zMjJ1Nb,oZ2pYN1,UFwnobA,vE2P7LJ,JiSHf7t","google":"","note":"Don't delete this file! It's used internally to help with page regeneration."}